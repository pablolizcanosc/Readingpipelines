{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"HI! I'm Pablo and my boss won't pay me\\n\\nThis is my story:\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googletrans import Translator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Token, Doc\n",
    "import pandas as pd\n",
    "import wordfreq\n",
    "import numpy as np\n",
    "import simplemma\n",
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "Token.set_extension('gram_features_', default={\"family\":\"\",\"subtype\":\"\",\"aux_labels\":[],\"aux_string\":\"\"},force=True)\n",
    "Token.set_extension(\"question_labels_\",default=[],force=True)\n",
    "Token.set_extension(\"newword_\",default=[],force=True)\n",
    "Token.set_extension(\"html_formatted_SN\", default=\"\",force=True)\n",
    "Token.set_extension(\"html_formatted_SV\", default=\"\",force=True)\n",
    "Token.set_extension(\"html_formatted_new\", default=\"\",force=True)\n",
    "Doc.set_extension(\"translation\", default=\"\",force=True)\n",
    "Doc.set_extension(\"html_formatted_SN\", default=\"\",force=True)\n",
    "Doc.set_extension(\"html_formatted_SV\", default=\"\",force=True)\n",
    "Doc.set_extension(\"html_formatted_new\", default=\"\",force=True)\n",
    "\n",
    "\n",
    "##################################################\n",
    "### Grammar\n",
    "##################################################\n",
    "def process_verb(token):\n",
    "\n",
    "    token._.gram_features_[\"aux_labels\"] = []\n",
    "\n",
    "    if token._.gram_features_[\"family\"]==\"\": #only words without previous category can become main verb\n",
    "        token._.gram_features_[\"family\"] = \"VERB\"\n",
    "        token._.gram_features_[\"subtype\"] = \"MAIN\"\n",
    "\n",
    "    morphoverb = token.morph.to_dict()\n",
    "    token._.gram_features_[\"aux_labels\"] += [morphoverb[i] for i in morphoverb if i in ['Mood','Tense', 'VerbForm']]\n",
    "\n",
    "    for child in token.children:\n",
    "        #separable verbs\n",
    "        if child.dep_ == \"svp\": \n",
    "            child._.gram_features_[\"family\"] = \"VERB\" \n",
    "            child._.gram_features_[\"subtype\"] = \"SVP\"\n",
    "            child._.question_labels_ = [\"SVP\"]\n",
    "\n",
    "            token._.gram_features_[\"aux_labels\"] += [\"SEPARABLE_VERB\"]\n",
    "            token._.gram_features_[\"aux_string\"] = child.text\n",
    "        #auxiliary verbs\n",
    "        elif child.pos_ == \"AUX\": \n",
    "            child._.gram_features_[\"family\"] = \"VERB\"\n",
    "            child._.gram_features_[\"subtype\"] = \"AUX\"\n",
    "        #Reflexive verbs\n",
    "        else: \n",
    "            checks=0\n",
    "            morphochild = child.morph.to_dict()\n",
    "            #Reflexive verb detected by pipeline\n",
    "            if morphochild.get(\"Reflex\",\"No\")==\"Yes\": \n",
    "                token._.gram_features_[\"aux_labels\"] += [\"REFLEXIVE_VERB\"]\n",
    "                child._.gram_features_[\"family\"] = \"VERB\"\n",
    "                child._.gram_features_[\"subtype\"] = \"REFLEXPRON\"\n",
    "            #Reflexive verb - number and person agreement with verb\n",
    "            elif (\"Number\" in morphoverb) and (\"Person\" in morphoverb): \n",
    "                if (child.pos_==\"PRON\") and (morphochild[\"PronType\"]==\"Prs\")and (morphochild[\"PronType\"] in [\"Acc\",\"Dat\"]):\n",
    "                    for check in [\"Number\",\"Person\"]:\n",
    "                        checks+=(morphochild[check]==morphoverb[check])\n",
    "                    if checks == 2:\n",
    "                        child._.gram_features_[\"family\"] = \"VERB\"\n",
    "                        child._.gram_features_[\"subtype\"] = \"REFLEXPRON\"\n",
    "                        token._.gram_features_[\"aux_labels\"] += [\"REFLEXIVE_VERB\"]\n",
    "\n",
    "\n",
    "def process_nonverb(token):\n",
    "    token._.gram_features_[\"aux_labels\"] = []\n",
    "    #noun type\n",
    "    if token.pos_ in [\"NOUN\", \"PRON\", \"PROPN\", \"ADJ\", \"DET\"]: \n",
    "        if token._.gram_features_[\"family\"] == \"\":\n",
    "            token._.gram_features_[\"family\"] = \"NOUN\"\n",
    "            token._.gram_features_[\"subtype\"] = token.pos_\n",
    "        morphodict = token.morph.to_dict()\n",
    "        token._.gram_features_[\"aux_labels\"] += [morphodict[i] for i in morphodict if i in [\"Gender\",\"Case\",\"Number\"]]\n",
    "    #others\n",
    "    elif token.pos_ not in [\"X\",\"SPACE\",\"PUNCT\"]:\n",
    "        if token._.gram_features_[\"family\"] == \"\":\n",
    "            token._.gram_features_[\"family\"] = \"OTHERS\"\n",
    "            token._.gram_features_[\"subtype\"] = token.pos_\n",
    "            token._.question_labels_ = [\"OTHERS\", token.pos_]\n",
    "\n",
    "@Language.component(\"grammarfeatures\")\n",
    "def process_all_tokens(doc):\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"VERB\",\"AUX\"]:\n",
    "            process_verb(token)\n",
    "        else:\n",
    "            process_nonverb(token)\n",
    "    return doc\n",
    "\n",
    "\n",
    "##################################################\n",
    "### VOCABULARY\n",
    "##################################################\n",
    "\n",
    "\n",
    "\n",
    "def basewordfreq(token):\n",
    "\n",
    "    if token.pos_ not in [\"NUM\",\"X\",\"PROPN\",\"SPACE\",\"PUNCT\"] and not ((token._.gram_features_[\"family\"] == \"VERB\") and (token._.gram_features_[\"subtype\"] == \"SVP\")):\n",
    "        word_candidates = [token.lemma_, token.text, simplemma.lemmatize(token.text, lang='de')]\n",
    "        \n",
    "        if \"SEPARABLE_VERB\" in token._.gram_features_[\"aux_labels\"]:\n",
    "            word_candidates = [token._.gram_features_[\"aux_string\"] + word_candidate for word_candidate in word_candidates] # particle zu in the middle? Check grammar\n",
    "        \n",
    "        maxfreq = max([wordfreq.word_frequency(i,\"de\") for i in word_candidates])\n",
    "                      \n",
    "        return token, token._.gram_features_[\"family\"], list(set(word_candidates)), maxfreq\n",
    "   \n",
    "    return token, np.nan, np.nan, np.nan\n",
    "\n",
    "def marknewword(token):\n",
    "\n",
    "    token._.newword_=True\n",
    "\n",
    "    if len(token._.question_labels_)==0:\n",
    "        token._.question_labels_ = [\"NEW\"]\n",
    "    else:\n",
    "        token._.question_labels_ += [\"NEW\"]\n",
    "\n",
    "    if (token._.gram_features_[\"family\"] == \"VERB\") and (token._.gram_features_[\"subtype\"] == \"MAIN\"):\n",
    "        for child in token.children:\n",
    "            if (child._.gram_features_[\"family\"] == \"VERB\") and (child._.gram_features_[\"subtype\"] in [\"REFLEXPRON\", \"SVP\"]):\n",
    "                child._.newword_ = True\n",
    "                if len(child._.question_labels_)==0:\n",
    "                    child._.question_labels_ = [\"NEW\"]\n",
    "                else: #controllare che new non c'è già\n",
    "                    token._.question_labels_ += [\"NEW\"]                \n",
    "\n",
    "\n",
    "@Language.component(\"vocabulary\")    \n",
    "def processvocabulary(doc):\n",
    "    history_path = \"userdata/\"+user + \"/history.csv\"\n",
    "    profile_path = \"userdata/\"+user + \"/profile.csv\"\n",
    "    try:\n",
    "        history = pd.read_csv(history_path,sep=\"|\", index_col = 0)\n",
    "        profile = pd.read_csv(profile_path,sep=\"|\", index_col = 0)\n",
    "        new_user = False\n",
    "    except:\n",
    "        new_user = True\n",
    "\n",
    "\n",
    "    dfvocab = pd.Series(doc).apply(basewordfreq).apply(pd.Series)\n",
    "    nr_words = len(dfvocab.dropna())\n",
    "    dfvocab = dfvocab.explode(2)\n",
    "        \n",
    "    dfvocab.columns = [\"token\", \"family\", \"base\", \"freq\"]\n",
    "\n",
    "    dfaux = dfvocab.groupby([\"family\", \"base\"]).agg({\"freq\":[\"count\",\"max\"]})\n",
    "    dfaux.columns = [\"nr_appearances\", \"freq_corpus\"]\n",
    "    dfaux=dfaux.reset_index()\n",
    "    dfaux[\"freq_text\"]=dfaux[\"nr_appearances\"]/nr_words\n",
    "    \n",
    "    if not new_user:\n",
    "        history.columns = [\"family\", \"base\", \"nr_appearances_old\", \"freq_corpus_old\",\"highlighted\"]\n",
    "        dfaux = pd.merge(dfaux,history, on= [\"family\",\"base\"], how = 'outer')\n",
    "        dfaux[\"nr_appearances\"] = dfaux[\"nr_appearances\"].fillna(0)\n",
    "        dfaux[\"nr_appearances_old\"] = dfaux[\"nr_appearances_old\"].fillna(0)\n",
    "        dfaux[\"nr_app_tot\"] = dfaux[\"nr_appearances\"]+dfaux[\"nr_appearances_old\"]\n",
    "        dfaux[\"highlight\"]=(dfaux[\"highlighted\"]==False)&((dfaux[\"nr_app_tot\"]>30)|((dfaux[\"freq_text\"]>0.01)&(dfaux[\"nr_appearances\"]>5)))\n",
    "    else:\n",
    "        dfaux[\"highlight\"]=(dfaux[\"freq_corpus\"]>0.01)|((dfaux[\"freq_text\"]>0.01)&(dfaux[\"nr_appearances\"]>5))\n",
    "        \n",
    "    if (dfaux[\"highlight\"].sum()<10):\n",
    "        if not new_user:\n",
    "            highlight = dfaux[(dfaux[\"highlight\"]==False)&(dfaux[\"highlighted\"]==False)].sort_values(\"freq_corpus\",ascending=False).iloc[:10-dfaux[\"highlight\"].sum()][[\"family\",\"base\"]].drop_duplicates()\n",
    "        else:\n",
    "            highlight = dfaux[dfaux[\"highlight\"]==False].sort_values(\"freq_corpus\",ascending=False).iloc[:10-dfaux[\"highlight\"].sum()][[\"family\",\"base\"]].drop_duplicates()\n",
    "        tohighlight = list(highlight[\"family\"]+\"_\"+highlight[\"base\"])\n",
    "        dfaux[\"highlight\"] = dfaux[\"highlight\"]|dfaux.apply(lambda x: x[\"family\"]+\"_\"+x[\"base\"] in tohighlight, axis = 1)\n",
    "\n",
    "    pd.merge(dfvocab,dfaux[dfaux[\"highlight\"]])[\"token\"].apply(marknewword)\n",
    "    \n",
    "    if not new_user:\n",
    "        dfaux[\"nr_appearances\"] = dfaux[\"nr_app_tot\"]\n",
    "        dfaux[\"freq_corpus\"] = dfaux.apply(lambda x: max([x[\"freq_corpus_old\"],x[\"freq_corpus\"]]),axis = 1)\n",
    "        dfaux[\"highlighted\"] = dfaux[\"highlighted\"]|dfaux[\"highlight\"]\n",
    "    else:\n",
    "        dfaux=dfaux[[\"family\",\"base\",\"nr_appearances\",\"freq_corpus\",\"highlight\"]]\n",
    "        dfaux.columns = [\"family\",\"base\",\"nr_appearances\",\"freq_corpus\",\"highlighted\"]\n",
    "    dfaux=dfaux[[\"family\",\"base\",\"nr_appearances\",\"freq_corpus\",\"highlighted\"]]\n",
    "    dfaux.to_csv(history_path,sep=\"|\")\n",
    "    \n",
    "    if not new_user:\n",
    "        profile.iloc[0,0]=profile.iloc[0,0]+1\n",
    "        profile.iloc[1,0]=profile.iloc[1,0]+nr_words\n",
    "        profile.to_csv(profile_path,sep=\"|\")\n",
    "    else:\n",
    "        pd.Series([1, nr_words]).to_csv(profile_path,sep=\"|\")\n",
    "    \n",
    "    return doc\n",
    "\n",
    "\n",
    "##################################################\n",
    "### FORMAT\n",
    "##################################################\n",
    "\n",
    "@Language.component(\"textformatter\")\n",
    "def formatsingletoken(token):\n",
    "    attributes = token._.gram_features_\n",
    "    formatted = token.text\n",
    "    formatted_v = token.text\n",
    "    if attributes[\"family\"]==\"VERB\":\n",
    "        formatted=\"<b>{}</b>\".format(formatted)\n",
    "        formatted_v=\"<b>{}</b>\".format(formatted_v)\n",
    "        if \"Fin\" in attributes[\"aux_labels\"]:\n",
    "            formatted_v ='<span style=\"color: blue\">{}</span>'.format(formatted_v)\n",
    "        if \"Part\" in attributes[\"aux_labels\"]:\n",
    "            formatted_v ='<span style=\"color: pink\">{}</span>'.format(formatted_v)\n",
    "        if \"Inf\" in attributes[\"aux_labels\"]:\n",
    "            formatted_v ='<span style=\"color: orange\">{}</span>'.format(formatted_v)\n",
    "        if \"Sub\" in attributes[\"aux_labels\"]:\n",
    "            formatted_v = '<span class=\"wavyUnderline\">{}</span>'.format(formatted_v)\n",
    "        if \"Ind\" in attributes[\"aux_labels\"]:\n",
    "            #formatted_v = '<span class=\"normalUnderline\">{}</span>'.format(formatted_v)\n",
    "            pass\n",
    "        if \"Pres\" in attributes[\"aux_labels\"]:\n",
    "            pass\n",
    "            #formatted_v = '<span class=\"normalOverline\">{}</span>'.format(formatted_v)\n",
    "        if \"Past\" in attributes[\"aux_labels\"]:\n",
    "            formatted_v = '<i>{}</i>'.format(formatted_v)\n",
    "    elif token._.gram_features_[\"family\"] == \"NOUN\": #prima cera un else e basta\n",
    "        #formatted_v = \"<b>{}</b>\".format(formatted_v)\n",
    "        if \"Nom\" in attributes[\"aux_labels\"]:\n",
    "            formatted = '<span class=\"doubleUnderline\">{}</span>'.format(formatted)\n",
    "            formatted_v = formatted\n",
    "        if \"Acc\" in attributes[\"aux_labels\"]:\n",
    "            formatted = '<span class=\"normalUnderline\">{}</span>'.format(formatted)\n",
    "            formatted_v = formatted\n",
    "        if \"Dat\" in attributes[\"aux_labels\"]:\n",
    "            formatted = '<span class=\"dashedUnderline\">{}</span>'.format(formatted)\n",
    "            formatted_v = formatted\n",
    "        if \"Gen\" in attributes[\"aux_labels\"]:\n",
    "            formatted = '<span class=\"dottedUnderline\">{}</span>'.format(formatted)\n",
    "\n",
    "        if \"Masc\" in attributes[\"aux_labels\"]:\n",
    "            formatted ='<span style=\"color: blue\">{}</span>'.format(formatted)\n",
    "        if \"Fem\" in attributes[\"aux_labels\"]:\n",
    "            formatted ='<span style=\"color: pink\">{}</span>'.format(formatted)\n",
    "        if \"Neut\" in attributes[\"aux_labels\"]:\n",
    "            formatted ='<span style=\"color: orange\">{}</span>'.format(formatted)\n",
    "            \n",
    "        if \"Plur\" in attributes[\"aux_labels\"]:\n",
    "            formatted ='<span class=\"doubleOverline\">{}</span>'.format(formatted)\n",
    "            if (\"Nom\" in attributes[\"aux_labels\"]) or (\"Dat\" in attributes[\"aux_labels\"]) or (\"Acc\" in attributes[\"aux_labels\"]):\n",
    "                formatted_v = '<span class=\"doubleOverline\">{}</span>'.format(formatted_v)\n",
    "        if \"Sing\" in attributes[\"aux_labels\"]:\n",
    "            formatted ='<span class=\"normalOverline\">{}</span>'.format(formatted)\n",
    "            #formatted_v = '<span class=\"normalOverline\">{}</span>'.format(formatted_v)\n",
    "    formatted_new = token.text\n",
    "    if token._.newword_:\n",
    "        formatted_new = \"<b>{}</b>\".format(formatted_new)\n",
    "\n",
    "\n",
    "    token._.html_formatted_SN= formatted\n",
    "    token._.html_formatted_SV= formatted_v\n",
    "    token._.html_formatted_new= formatted_new\n",
    "\n",
    "@Language.component(\"translation\")\n",
    "def gettranslatednumbered(text_with_n):\n",
    "\n",
    "    translator = Translator()\n",
    "    final = \"\"\n",
    "    final_t = \"\"\n",
    "    elements = text_with_n.split(\"\\n\")\n",
    "    nr_el = len(elements)\n",
    "    ciphers = len(str(nr_el))\n",
    "    for i in range(nr_el):\n",
    "        if elements[i]!=\"\":\n",
    "            final+= \"(\"+str(i).zfill(ciphers)+\") \"+elements[i]+\"</br>\"\n",
    "            final_t+= \"(\"+str(i).zfill(ciphers)+\") \"+translator.translate(elements[i]).text+\"</br>\"\n",
    "        else:\n",
    "            final+=\"</br>\"\n",
    "            final_t+=\"</br>\"\n",
    "    return final,final_t\n",
    "\n",
    "@Language.component(\"format\")\n",
    "def computeformat(doc):\n",
    "    \n",
    "    html_sn = \"\"\n",
    "    html_sv = \"\"\n",
    "    html_new = \"\"\n",
    "\n",
    "    for token in doc:\n",
    "\n",
    "        formatsingletoken(token)\n",
    "\n",
    "        if token.is_punct==True:\n",
    "            html_sn += token._.html_formatted_SN\n",
    "            html_sv += token._.html_formatted_SV\n",
    "            html_new += token._.html_formatted_new\n",
    "        else:\n",
    "            html_sn += \" \"+ token._.html_formatted_SN\n",
    "            html_sv += \" \"+ token._.html_formatted_SV\n",
    "            html_new += \" \"+ token._.html_formatted_new\n",
    "    \n",
    "    doc._.html_formatted_SN = html_sn\n",
    "    doc._.html_formatted_SV = html_sv\n",
    "    doc._.html_formatted_new = html_new\n",
    "\n",
    "    return doc\n",
    "\n",
    "@Language.component(\"translate\")\n",
    "def translate(doc):\n",
    "    doc._.translation = translator.translate(doc.text).text\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pablo\\Documents\\clean_projects\\Readingpipelines\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.translate(doc)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = \"pablo\"\n",
    "nlp = spacy.load('de_dep_news_trf')\n",
    "nlp.add_pipe(\"grammarfeatures\")\n",
    "nlp.add_pipe(\"vocabulary\")\n",
    "nlp.add_pipe(\"format\")\n",
    "nlp.add_pipe(\"translate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "In der Türkei läuft die entscheidende Stichwahl um das Präsidentenamt. Die Wahllokale öffneten am Sonntagmorgen (Ortszeit). Wahlberechtigt sind insgesamt mehr als 64 Millionen Türken. Rund dreieinhalb Millionen im Ausland lebende Staatsbürger konnten bereits zwischen dem 20. und 24. Mai abstimmen. Es ist die erste Stichwahl in der Geschichte des Landes.\n",
    "\n",
    "Der islamisch-konservative Staatschef Recep Tayyip Erdogan hatte in der ersten Wahlrunde vor zwei Wochen deutlich besser abgeschnitten als von Meinungsforschern erwartet, verfehlte mit 49,5 Prozent der Stimmen aber knapp die für einen Sieg erforderliche absolute Mehrheit. Sein sozialdemokratischer Herausforderer Kemal Kilicdaroglu kam auf 44,9 Prozent.\n",
    "\n",
    "Vor dem ersten Durchgang am 14. Mai waren Oppositionsführer Kilicdaroglu, der an der Spitze eines Sechs-Parteien-Bündnisses antritt, gute Siegeschancen zugesprochen worden. In der Stichwahl gilt nun aber Erdogan als klarer Favorit, zumal der drittplatzierte Kandidat Sinan Ogan eine Wahlempfehlung für den Amtsinhaber aussprach.\n",
    "\n",
    "Richtungsweisende Wahl\n",
    "Erdogan (69) ist bereits seit 20 Jahren an der Macht. Kritiker befürchten, dass die Türkei mit ihren rund 85 Millionen Einwohnern vollends in die Autokratie abgleiten könnte, sollte er erneut gewinnen. Kilicdaroglu (74) versprach, das Land zu demokratisieren.\n",
    "\n",
    "Zuletzt hatte das Thema Migration den Wahlkampf bestimmt. Vor allem Kilicdaroglu drängte auf die Rückführung von Flüchtlingen nach Syrien. Weiteres Thema war die schlechte wirtschaftliche Lage mit einer massiven Inflation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Prinzipiell findet Maximilian Pichl von der Universität Kassel das auch gut: Im Sinne einer effektiven humanitären Hilfe sei ein schnelles Handeln der EU wichtig, \"zumal sie in der Vergangen­heit bei der Aufnahme von Flüchtlingen oft uneinheitlich und repressiv vorging\", schreibt der Rechts- und Politikwissenschaftler im Grundrechte-Report, jährlich veröffentlicht seit 1997 von Bürgerrechts- und Menschenrechtsorganisationen. Dazu gehören Pro Asyl, die Humanistische Union und die Internationale Liga für Menschrechte.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Turkey, the decisive run-off election for the presidency is underway. The polling stations opened on Sunday morning (local time). A total of more than 64 million Turks are entitled to vote. Around three and a half million citizens living abroad were able to vote between May 20th and 24th. It is the first runoff election in the country's history.\n",
      "\n",
      "The Islamic-conservative head of state Recep Tayyip Erdogan did significantly better than pollsters had expected in the first round of the election two weeks ago, but with 49.5 percent of the votes just missed the absolute majority required for victory. His Social Democratic challenger Kemal Kilicdaroglu got 44.9 percent.\n",
      "\n",
      "Before the first round on May 14, opposition leader Kilicdaroglu, who is leading a six-party alliance, had been given good chances of victory. However, Erdogan is now the clear favorite in the run-off election, especially since the third-placed candidate, Sinan Ogan, made a recommendation for the incumbent.\n",
      "\n",
      "Pioneering choice\n",
      "Erdogan (69) has been in power for 20 years. Critics fear that Turkey, with its approximately 85 million inhabitants, could slide completely into autocracy if he wins again. Kilicdaroglu (74) promised to democratize the country.\n",
      "\n",
      "Most recently, the issue of migration had dominated the election campaign. Kilicdaroglu in particular pushed for the return of refugees to Syria. Another topic was the poor economic situation with massive inflation.\n"
     ]
    }
   ],
   "source": [
    "print(doc._.translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    #Token.set_extension('svp', default=\"\",force=True)\n",
    "    Token.set_extension(\"html_formatted_SN\", default=\"\",force=True)\n",
    "    Token.set_extension(\"html_formatted_SV\", default=\"\",force=True)\n",
    "    Token.set_extension(\"html_formatted_new\", default=\"\",force=True)\n",
    "    Token.set_extension(\"newword\",default=False,force=True)\n",
    "    nlp = spacy.load(\"de_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "########## MEANING AND GRAMMAR\n",
    "############################################################\n",
    "\n",
    "from googletrans import Translator\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import random\n",
    "from datetime import date\n",
    "import sys\n",
    "import codecs\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "# new step pipeline https://spacy.io/usage/processing-pipelines\n",
    "def process_verb(token):\n",
    "\n",
    "    token._.gram_features_[\"aux_labels\"] = []\n",
    "\n",
    "    if token._.gram_features_[\"family\"]==\"\": #only words without previous category can become main verb\n",
    "        token._.gram_features_[\"family\"] = \"VERB\"\n",
    "        token._.gram_features_[\"subtype\"] = \"MAIN\"\n",
    "\n",
    "    morphoverb = token.morph.to_dict()\n",
    "    token._.gram_features_[\"aux_labels\"] += [morphoverb[i] for i in morphoverb if i in ['Mood','Tense', 'VerbForm']]\n",
    "\n",
    "    for child in token.children:\n",
    "        #separable verbs\n",
    "        if child.dep_ == \"svp\": \n",
    "            child._.gram_features_[\"family\"] = \"VERB\" \n",
    "            child._.gram_features_[\"subtype\"] = \"SVP\"\n",
    "            child._.question_labels_ = [\"SVP\"]\n",
    "\n",
    "            token._.gram_features_[\"aux_labels\"] += [\"SEPARABLE_VERB\"]\n",
    "            token._.gram_features_[\"aux_string\"] = child.text\n",
    "        #auxiliary verbs\n",
    "        elif child.pos_ == \"AUX\": \n",
    "            child._.gram_features_[\"family\"] = \"VERB\"\n",
    "            child._.gram_features_[\"subtype\"] = \"AUX\"\n",
    "        #Reflexive verbs\n",
    "        else: \n",
    "            checks=0\n",
    "            morphochild = child.morph.to_dict()\n",
    "            #Reflexive verb detected by pipeline\n",
    "            if morphochild.get(\"Reflex\",\"No\")==\"Yes\": \n",
    "                token._.gram_features_[\"aux_labels\"] += [\"REFLEXIVE_VERB\"]\n",
    "                child._.gram_features_[\"family\"] = \"VERB\"\n",
    "                child._.gram_features_[\"subtype\"] = \"REFLEXPRON\"\n",
    "            #Reflexive verb - number and person agreement with verb\n",
    "            elif (\"Number\" in morphoverb) and (\"Person\" in morphoverb): \n",
    "                if (child.pos_==\"PRON\") and (morphochild[\"PronType\"]==\"Prs\")and (morphochild[\"PronType\"] in [\"Acc\",\"Dat\"]):\n",
    "                    for check in [\"Number\",\"Person\"]:\n",
    "                        checks+=(morphochild[check]==morphoverb[check])\n",
    "                    if checks == 2:\n",
    "                        child._.gram_features_[\"family\"] = \"VERB\"\n",
    "                        child._.gram_features_[\"subtype\"] = \"REFLEXPRON\"\n",
    "                        token._.gram_features_[\"aux_labels\"] += [\"REFLEXIVE_VERB\"]\n",
    "\n",
    "\n",
    "def process_nonverb(token):\n",
    "    token._.gram_features_[\"aux_labels\"] = []\n",
    "    #noun type\n",
    "    if token.pos_ in [\"NOUN\", \"PRON\", \"PROPN\", \"ADJ\", \"DET\"]: \n",
    "        if token._.gram_features_[\"family\"] == \"\":\n",
    "            token._.gram_features_[\"family\"] = \"NOUN\"\n",
    "            token._.gram_features_[\"subtype\"] = token.pos_\n",
    "        morphodict = token.morph.to_dict()\n",
    "        token._.gram_features_[\"aux_labels\"] += [morphodict[i] for i in morphodict if i in [\"Gender\",\"Case\",\"Number\"]]\n",
    "    #others\n",
    "    elif token.pos_ not in [\"X\",\"SPACE\",\"PUNCT\"]:\n",
    "        if token._.gram_features_[\"family\"] == \"\":\n",
    "            token._.gram_features_[\"family\"] = \"OTHERS\"\n",
    "            token._.gram_features_[\"subtype\"] = token.pos_\n",
    "            token._.question_labels_ = [\"OTHERS\", token.pos_]\n",
    "\n",
    "def process_all_tokens(doc):\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"VERB\",\"AUX\"]:\n",
    "            process_verb(token)\n",
    "        else:\n",
    "            process_nonverb(token)\n",
    "    return doc\n",
    "\n",
    "\n",
    "\n",
    "def gettranslatednumbered(text_with_n):\n",
    "\n",
    "    translator = Translator()\n",
    "    final = \"\"\n",
    "    final_t = \"\"\n",
    "    elements = text_with_n.split(\"\\n\")\n",
    "    nr_el = len(elements)\n",
    "    ciphers = len(str(nr_el))\n",
    "    for i in range(nr_el):\n",
    "        if elements[i]!=\"\":\n",
    "            final+= \"(\"+str(i).zfill(ciphers)+\") \"+elements[i]+\"</br>\"\n",
    "            final_t+= \"(\"+str(i).zfill(ciphers)+\") \"+translator.translate(elements[i]).text+\"</br>\"\n",
    "        else:\n",
    "            final+=\"</br>\"\n",
    "            final_t+=\"</br>\"\n",
    "    return final,final_t\n",
    "\n",
    "def numberparagraphs(text_with_n):\n",
    "    final = \"\"\n",
    "    elements = text_with_n.split(\"\\n\")\n",
    "    nr_el = len(elements)\n",
    "    ciphers = len(str(nr_el))\n",
    "    for i in range(nr_el):\n",
    "        if elements[i]!=\"\":\n",
    "            final+= \"(\"+str(i).zfill(ciphers)+\") \"+elements[i]+\"</br>\"\n",
    "        else:\n",
    "            final+=\"</br>\"\n",
    "    return final\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################\n",
    "### VOCABULARY\n",
    "##################################################\n",
    "\n",
    "import wordfreq\n",
    "import numpy as np\n",
    "import simplemma\n",
    "langdata = simplemma.load_data('de')\n",
    "\n",
    "def basewordfreq(x):\n",
    "    if x.pos_ not in [\"NUM\",\"X\",\"PROPN\",\"SPACE\",\"PUNCT\"] and not ((x._.gram_features_[\"family\"] == \"VERB\") and (x._.gram_features_[\"subtype\"] == \"SVP\")):\n",
    "        word_candidates = [x.lemma_, x.text, simplemma.lemmatize(x.text, langdata)]\n",
    "        if \"SEPARABLE_VERB\" in x._.gram_features_[\"aux_labels\"]:\n",
    "            word_candidates = [x._.gram_features_[\"aux_string\"]+i for i in word_candidates]\n",
    "        maxfreq = max([wordfreq.word_frequency(i,\"de\") for i in word_candidates])\n",
    "                      \n",
    "        return x, x._.gram_features_[\"family\"], list(set(word_candidates)), maxfreq\n",
    "    return x, np.nan, np.nan, np.nan\n",
    "\n",
    "def isanewword(x):\n",
    "    x._.newword=True\n",
    "    if len(x._.question_labels_)==0:\n",
    "        x._.question_labels_ = [\"NEW\"]\n",
    "    else:\n",
    "        x._.question_labels_ += [\"NEW\"]\n",
    "    if (x._.gram_features_[\"family\"] == \"VERB\") and (x._.gram_features_[\"subtype\"] == \"MAIN\"):\n",
    "        for i in x.children:\n",
    "            if (i._.gram_features_[\"family\"] == \"VERB\") and (i._.gram_features_[\"subtype\"] in [\"REFLEXPRON\", \"SVP\"]):\n",
    "                i._.newword = True\n",
    "                if len(i._.question_labels_)==0:\n",
    "                    i._.question_labels_ = [\"NEW\"]\n",
    "                else: #controllare che new non c'è già\n",
    "                    x._.question_labels_ += [\"NEW\"]                \n",
    "    \n",
    "def processvocabulary(doc):\n",
    "    try:\n",
    "        history = pd.read_csv(\"history.csv\",sep=\"|\", index_col = 0)\n",
    "        profile = pd.read_csv(\"profile.csv\",sep=\"|\", index_col = 0)\n",
    "\n",
    "        dfvocab = pd.Series(doc).apply(basewordfreq).apply(pd.Series)\n",
    "        nr_words = len(dfvocab.dropna())\n",
    "        dfvocab = dfvocab.explode(2)\n",
    "        \n",
    "        dfvocab.columns = [\"token\", \"family\", \"base\", \"freq\"]\n",
    "\n",
    "        dfaux = dfvocab.groupby([\"family\", \"base\"]).agg({\"freq\":[\"count\",\"max\"]})\n",
    "        dfaux.columns = [\"nr_appearances\", \"freq_corpus\"]\n",
    "        dfaux=dfaux.reset_index()\n",
    "        dfaux[\"freq_text\"]=dfaux[\"nr_appearances\"]/nr_words\n",
    "        \n",
    "        history.columns = [\"family\", \"base\", \"nr_appearances_old\", \"freq_corpus_old\",\"highlighted\"]\n",
    "        dfaux = pd.merge(dfaux,history, on= [\"family\",\"base\"], how = 'outer')\n",
    "        dfaux[\"nr_appearances\"] = dfaux[\"nr_appearances\"].fillna(0)\n",
    "        dfaux[\"nr_appearances_old\"] = dfaux[\"nr_appearances_old\"].fillna(0)\n",
    "        dfaux[\"nr_app_tot\"] = dfaux[\"nr_appearances\"]+dfaux[\"nr_appearances_old\"]\n",
    "        dfaux[\"highlight\"]=(dfaux[\"highlighted\"]==False)&((dfaux[\"nr_app_tot\"]>30)|((dfaux[\"freq_text\"]>0.01)&(dfaux[\"nr_appearances\"]>5)))\n",
    "        if (dfaux[\"highlight\"].sum()<10):\n",
    "            highlight = dfaux[(dfaux[\"highlight\"]==False)&(dfaux[\"highlighted\"]==False)].sort_values(\"freq_corpus\",ascending=False).iloc[:10-dfaux[\"highlight\"].sum()][[\"family\",\"base\"]].drop_duplicates()\n",
    "            tohighlight = list(highlight[\"family\"]+\"_\"+highlight[\"base\"])\n",
    "            dfaux[\"highlight\"] = dfaux[\"highlight\"]|dfaux.apply(lambda x: x[\"family\"]+\"_\"+x[\"base\"] in tohighlight, axis = 1)\n",
    "        pd.merge(dfvocab,dfaux[dfaux[\"highlight\"]])[\"token\"].apply(isanewword)\n",
    "        dfaux[\"nr_appearances\"] = dfaux[\"nr_app_tot\"]\n",
    "        dfaux[\"freq_corpus\"] = dfaux.apply(lambda x: max([x[\"freq_corpus_old\"],x[\"freq_corpus\"]]),axis = 1)\n",
    "        dfaux[\"highlighted\"] = dfaux[\"highlighted\"]|dfaux[\"highlight\"]\n",
    "        dfaux=dfaux[[\"family\",\"base\",\"nr_appearances\",\"freq_corpus\",\"highlighted\"]]\n",
    "        dfaux.to_csv(\"history.csv\",sep=\"|\")\n",
    "        \n",
    "        profile.iloc[0,0]=profile.iloc[0,0]+1\n",
    "        profile.iloc[1,0]=profile.iloc[1,0]+nr_words\n",
    "        profile.to_csv(\"profile.csv\",sep=\"|\")\n",
    "    except:\n",
    "        dfvocab = pd.Series(doc).apply(basewordfreq).apply(pd.Series)\n",
    "        nr_words = len(dfvocab.dropna())\n",
    "        dfvocab = dfvocab.explode(2)\n",
    "        dfvocab.columns = [\"token\", \"family\", \"base\", \"freq\"]\n",
    "        dfaux = dfvocab.groupby([\"family\", \"base\"]).agg({\"freq\":[\"count\",\"max\"]})\n",
    "        dfaux.columns = [\"nr_appearances\", \"freq_corpus\"]\n",
    "        dfaux=dfaux.reset_index()\n",
    "        dfaux[\"freq_text\"]=dfaux[\"nr_appearances\"]/nr_words\n",
    "        dfaux[\"highlight\"]=(dfaux[\"freq_corpus\"]>0.01)|((dfaux[\"freq_text\"]>0.01)&(dfaux[\"nr_appearances\"]>5))\n",
    "        if (dfaux[\"highlight\"].sum()<10):\n",
    "            highlight = dfaux[dfaux[\"highlight\"]==False].sort_values(\"freq_corpus\",ascending=False).iloc[:10-dfaux[\"highlight\"].sum()][[\"family\",\"base\"]].drop_duplicates()\n",
    "            tohighlight = list(highlight[\"family\"]+\"_\"+highlight[\"base\"])\n",
    "            dfaux[\"highlight\"] = dfaux[\"highlight\"]|dfaux.apply(lambda x: x[\"family\"]+\"_\"+x[\"base\"] in tohighlight, axis =1)\n",
    "        pd.merge(dfvocab,dfaux[dfaux[\"highlight\"]])[\"token\"].apply(isanewword)\n",
    "        dfaux=dfaux[[\"family\",\"base\",\"nr_appearances\",\"freq_corpus\",\"highlight\"]]\n",
    "        dfaux.columns = [\"family\",\"base\",\"nr_appearances\",\"freq_corpus\",\"highlighted\"]\n",
    "        dfaux.to_csv(\"history.csv\",sep=\"|\")\n",
    "        pd.Series([1, nr_words]).to_csv(\"profile.csv\",sep=\"|\")\n",
    "        \n",
    "        \n",
    "##################################\n",
    "## FORMAT\n",
    "##################################\n",
    "\n",
    "def formattokensSNSV(x):\n",
    "    attributes = x._.gram_features_\n",
    "    formatted = x.text\n",
    "    formatted_v = x.text\n",
    "    if attributes[\"family\"]==\"VERB\":\n",
    "        formatted=\"<b>{}</b>\".format(formatted)\n",
    "        formatted_v=\"<b>{}</b>\".format(formatted_v)\n",
    "        if \"Fin\" in attributes[\"aux_labels\"]:\n",
    "            formatted_v ='<span style=\"color: blue\">{}</span>'.format(formatted_v)\n",
    "        if \"Part\" in attributes[\"aux_labels\"]:\n",
    "            formatted_v ='<span style=\"color: pink\">{}</span>'.format(formatted_v)\n",
    "        if \"Inf\" in attributes[\"aux_labels\"]:\n",
    "            formatted_v ='<span style=\"color: orange\">{}</span>'.format(formatted_v)\n",
    "        if \"Sub\" in attributes[\"aux_labels\"]:\n",
    "            formatted_v = '<span class=\"wavyUnderline\">{}</span>'.format(formatted_v)\n",
    "        if \"Ind\" in attributes[\"aux_labels\"]:\n",
    "            #formatted_v = '<span class=\"normalUnderline\">{}</span>'.format(formatted_v)\n",
    "            pass\n",
    "        if \"Pres\" in attributes[\"aux_labels\"]:\n",
    "            pass\n",
    "            #formatted_v = '<span class=\"normalOverline\">{}</span>'.format(formatted_v)\n",
    "        if \"Past\" in attributes[\"aux_labels\"]:\n",
    "            formatted_v = '<i>{}</i>'.format(formatted_v)\n",
    "    elif x._.gram_features_[\"family\"] == \"NOUN\": #prima cera un else e basta\n",
    "        #formatted_v = \"<b>{}</b>\".format(formatted_v)\n",
    "        if \"Nom\" in attributes[\"aux_labels\"]:\n",
    "            formatted = '<span class=\"doubleUnderline\">{}</span>'.format(formatted)\n",
    "            formatted_v = formatted\n",
    "        if \"Acc\" in attributes[\"aux_labels\"]:\n",
    "            formatted = '<span class=\"normalUnderline\">{}</span>'.format(formatted)\n",
    "            formatted_v = formatted\n",
    "        if \"Dat\" in attributes[\"aux_labels\"]:\n",
    "            formatted = '<span class=\"dashedUnderline\">{}</span>'.format(formatted)\n",
    "            formatted_v = formatted\n",
    "        if \"Gen\" in attributes[\"aux_labels\"]:\n",
    "            formatted = '<span class=\"dottedUnderline\">{}</span>'.format(formatted)\n",
    "\n",
    "        if \"Masc\" in attributes[\"aux_labels\"]:\n",
    "            formatted ='<span style=\"color: blue\">{}</span>'.format(formatted)\n",
    "        if \"Fem\" in attributes[\"aux_labels\"]:\n",
    "            formatted ='<span style=\"color: pink\">{}</span>'.format(formatted)\n",
    "        if \"Neut\" in attributes[\"aux_labels\"]:\n",
    "            formatted ='<span style=\"color: orange\">{}</span>'.format(formatted)\n",
    "            \n",
    "        if \"Plur\" in attributes[\"aux_labels\"]:\n",
    "            formatted ='<span class=\"doubleOverline\">{}</span>'.format(formatted)\n",
    "            if (\"Nom\" in attributes[\"aux_labels\"]) or (\"Dat\" in attributes[\"aux_labels\"]) or (\"Acc\" in attributes[\"aux_labels\"]):\n",
    "                formatted_v = '<span class=\"doubleOverline\">{}</span>'.format(formatted_v)\n",
    "        if \"Sing\" in attributes[\"aux_labels\"]:\n",
    "            formatted ='<span class=\"normalOverline\">{}</span>'.format(formatted)\n",
    "            #formatted_v = '<span class=\"normalOverline\">{}</span>'.format(formatted_v)\n",
    "    formatted_new = x.text\n",
    "    if x._.newword_:\n",
    "        formatted_new = \"<b>{}</b>\".format(formatted_new)\n",
    "\n",
    "\n",
    "    x._.html_formatted_SN= formatted\n",
    "    x._.html_formatted_SV= formatted_v\n",
    "    x._.html_formatted_new= formatted_new\n",
    "\n",
    "def computeformat(doc):\n",
    "    for i in doc:\n",
    "        formattokensSNSV(i)\n",
    "        \n",
    "def replacequestion(doc,qcat,qcatno=\"\"): #aggiungere una probabilità di sostituire oppure no.\n",
    "    string = \"\"\n",
    "    wordlist = []\n",
    "    for i in doc:\n",
    "        if i.is_punct == True:\n",
    "            string+=i.text\n",
    "        else:\n",
    "            if i.is_alpha and (qcat in i._.question_labels_) and (qcatno not in i._.question_labels_):\n",
    "                string+= \" ________ \"\n",
    "                wordlist.append(i.text.lower())\n",
    "            else:\n",
    "                string+= \" \"+i.text\n",
    "    random.shuffle(wordlist)\n",
    "\n",
    "    string += \"\"\"\n",
    "\n",
    "    You should use the words in the following list:\n",
    "\n",
    "    \"\"\" + str(wordlist)\n",
    "    return string\n",
    "\n",
    "def printFormattedText(doc):\n",
    "    initialstring = \"\"\n",
    "    css = \"\"\"\n",
    "    /* css here */\n",
    "\n",
    "{\n",
    "  border: 1px dotted black;\n",
    "}\n",
    "\n",
    "p.question {\n",
    "  font-family: Arial, sans-serif;\n",
    "  font-size:20px;\n",
    "  color: #2E2E2E;\n",
    "  margin-bottom:0px;\n",
    "}\n",
    "\n",
    "h2.quizHeader {\n",
    "  font-family: Arial, sans-serif;\n",
    "  font-weight:normal;\n",
    "  font-size:25px;\n",
    "  line-height: 27px;\n",
    "  margin: 24px 0 12px 0;\n",
    "  padding: 0 0 4px 0;\n",
    "  border-bottom: 1px solid #a2a2a2;\n",
    "}\n",
    "\n",
    "h2.quizScore{\n",
    "  font-family: Arial, sans-serif;\n",
    "  font-size:25px;\n",
    "}\n",
    "\n",
    "div.quizAnswers{\n",
    "  font-family: Arial, sans-serif;\n",
    "  font-size:16px;\n",
    "  color: #424242;\n",
    "  padding: 4px 0 4px 0;\n",
    "}\n",
    "\n",
    "label {\n",
    "  font-family: Arial, sans-serif;\n",
    "  font-size:14px;\n",
    "  color: #424242;\n",
    "  vertical-align:top;\n",
    "}\n",
    "\n",
    "input.answer[type=\"radio\"] {\n",
    "  margin-bottom: 10px;\n",
    "}\n",
    "\n",
    "input.quizSubmit[type=\"submit\"] {\n",
    "  -webkit-background-clip: border-box;\n",
    "  -webkit-background-origin: padding-box;\n",
    "  -webkit-background-size: auto;\n",
    "  -webkit-transition-delay: 0s, 0s;\n",
    "  -webkit-transition-duration: 0.2s, 0.2s;\n",
    "  -webkit-transition-property: color, background-color;\n",
    "  -webkit-transition-timing-function: ease, ease;\n",
    "  box-shadow: rgba(0, 0, 0, 0.498039) 0px 0px 5px 0px;\n",
    "  color: #ffffff;\n",
    "  background-color: #c30b0a;\n",
    "  margin: 0;\n",
    "  border: 0;\n",
    "  outline: 0;\n",
    "  text-transform:uppercase;\n",
    "  height:35px;\n",
    "  width:85px;\n",
    "  border: 1px solid #5E5E5E;\n",
    "  border-radius:5px;\n",
    "\n",
    " }\n",
    "\n",
    "input.quizSubmit[type=\"submit\"]:hover {\n",
    "  color: #ffffff;\n",
    "  background: #680f11;\n",
    "  text-decoration: none;\n",
    "}\n",
    "\n",
    "table {\n",
    "  background-color: #F2F2F2;\n",
    "  border:1px solid #BDBDBD;\n",
    "  border-radius:5px;\n",
    "  padding:10px;\n",
    "  padding-left:25px;\n",
    "  box-shadow: rgba(0, 0, 0, 0.498039) 0px 0px 1px 0px;\n",
    "}\n",
    "\n",
    "th {\n",
    "\n",
    "}\n",
    "\n",
    "tr {\n",
    "\n",
    "}\n",
    "\n",
    "td {\n",
    "\n",
    "}\n",
    "\n",
    ".submitter {\n",
    "\t  width:85px;\n",
    "}\n",
    "\n",
    ".hide {\n",
    "\t  display:none;\n",
    "}\n",
    ".normalUnderline {\n",
    "    text-decoration:underline #000;\n",
    "}\n",
    "\n",
    ".normalOverline {\n",
    "    text-decoration: overline 1px #000;\n",
    "}\n",
    "\n",
    ".wavyUnderline {\n",
    "    text-decoration:underline wavy #000;\n",
    "}\n",
    "\n",
    ".wavyOverline {\n",
    "    text-decoration: overline wavy 1px #000;\n",
    "}\n",
    "\n",
    ".doubleOverline {\n",
    "    text-decoration: overline 1px double #000;\n",
    "}\n",
    "\n",
    ".doubleUnderline {\n",
    "    text-decoration:underline #000;\n",
    "    border-bottom: 1px solid #000;\n",
    "}\n",
    "\n",
    ".dottedUnderline {\n",
    "    border-bottom: 1px dotted #000;\n",
    "    text-decoration: none;\n",
    "}\n",
    "\n",
    ".dashedUnderline {\n",
    "    border-bottom: 1px dashed #000;\n",
    "    text-decoration: none;\n",
    "}\n",
    "/*SFS light red = #c30b0a;\n",
    "SFS dark red = #9f2026; */\n",
    "\n",
    "    \"\"\"\n",
    "    initialstring_v = \"\"\n",
    "    initialstring_new = \"\"\n",
    "    initialstring_qsvp = replacequestion(doc,\"SVP\")\n",
    "    initialstring_qnew = replacequestion(doc,\"NEW\")\n",
    "    initialstring_qadp = replacequestion(doc,\"ADP\")\n",
    "    initialstring_qoth = replacequestion(doc,\"OTHERS\",\"ADP\")\n",
    "\n",
    "        \n",
    "                \n",
    "    \n",
    "    html = '''<!DOCTYPE html><html lang=\"en\"><head><style>{}</style></head><body><p style=\"line-height: 200%\">{}</p></body></html>'''\n",
    "\n",
    "    for i in doc:\n",
    "        if i.is_punct==True:\n",
    "            initialstring+=i._.html_formatted_SN\n",
    "            initialstring_v += i._.html_formatted_SV\n",
    "            initialstring_new += i._.html_formatted_new\n",
    "        else:\n",
    "            initialstring+= \" \"+ i._.html_formatted_SN\n",
    "            initialstring_v +=\" \"+ i._.html_formatted_SV\n",
    "            initialstring_new += \" \"+ i._.html_formatted_new\n",
    "\n",
    "\n",
    "\n",
    "    final,final_t = gettranslatednumbered(doc.text)\n",
    "    html3 = html.format(css,numberparagraphs(initialstring))\n",
    "    html4 = html.format(css,numberparagraphs(initialstring_v))\n",
    "    html5 = html.format(css,numberparagraphs(initialstring_new))\n",
    "    html6 = html.format(css,numberparagraphs(initialstring_qsvp))\n",
    "    html7 = html.format(css,numberparagraphs(initialstring_qnew))\n",
    "    html8 = html.format(css,numberparagraphs(initialstring_qadp))\n",
    "    html9 = html.format(css,numberparagraphs(initialstring_qoth))\n",
    "    html1 = html.format(css,final)\n",
    "    html2 = html.format(css,final_t)\n",
    "    return [html1, html2, html3, html4, html5, html6,html7,html8, html9]\n",
    "        \n",
    "        \n",
    "##################################\n",
    "## PIPELINE\n",
    "##################################\n",
    "\n",
    "\n",
    "def pipeline(folder, file, questions = False):\n",
    "    print(\"1\")\n",
    "    Token.set_extension('att_li', default={\"family\":\"\",\"subtype\":\"\",\"aux_labels\":[],\"aux_string\":\"\"},force=True)\n",
    "    #Token.set_extension('svp', default=\"\",force=True)\n",
    "    Token.set_extension(\"html_formatted_SN\", default=\"\",force=True)\n",
    "    Token.set_extension(\"html_formatted_SV\", default=\"\",force=True)\n",
    "    Token.set_extension(\"html_formatted_new\", default=\"\",force=True)\n",
    "    Token.set_extension(\"questioncategory\",default=[],force=True)\n",
    "    Token.set_extension(\"newword\",default=False,force=True)\n",
    "    nlp = spacy.load(\"de_core_news_lg\")\n",
    "    print(\"2\")\n",
    "\n",
    "    f = codecs.open(folder+\"/\"+file, \"r\", \"utf-8\")    # suess_sweet.txt file contains two\n",
    "    text = f.read()                          # comma-separated words: süß, sweet\n",
    "    f.close()\n",
    "\n",
    "    doc = nlp(text)\n",
    "    assert doc.has_annotation(\"SENT_START\")\n",
    "    computeattli(doc)\n",
    "    computeformat(doc)\n",
    "    return printFormattedText(doc)\n",
    "\n",
    "\n",
    "def pipeline2(text, folder = \"Templates\"):\n",
    "    global doc\n",
    "    print(\"1\")\n",
    "    Token.set_extension('att_li', default={\"family\":\"\",\"subtype\":\"\",\"aux_labels\":[],\"aux_string\":\"\"},force=True)\n",
    "    #Token.set_extension('svp', default=\"\",force=True)\n",
    "    Token.set_extension(\"html_formatted_SN\", default=\"\",force=True)\n",
    "    Token.set_extension(\"html_formatted_SV\", default=\"\",force=True)\n",
    "    Token.set_extension(\"html_formatted_new\", default=\"\",force=True)\n",
    "    Token.set_extension(\"questioncategory\",default=\"\",force=True)\n",
    "    Token.set_extension(\"newword\",default=False,force=True)\n",
    "    nlp = spacy.load(\"de_core_news_lg\")\n",
    "    print(\"2\")\n",
    "\n",
    "    doc = nlp(text)\n",
    "    assert doc.has_annotation(\"SENT_START\")\n",
    "    computeattli(doc)\n",
    "    \n",
    "    processvocabulary(doc)\n",
    "    computeformat(doc)\n",
    "    htmls =  printFormattedText(doc)\n",
    "    \n",
    "    fw = open(\"original.html\", \"w\")\n",
    "    fw.write(htmls[0])\n",
    "    fw.close()\n",
    "    \n",
    "    fw = open(\"translation.html\", \"w\")\n",
    "    fw.write(htmls[1])\n",
    "    fw.close()\n",
    "    \n",
    "    fw = open(\"SN.html\", \"w\")\n",
    "    fw.write(htmls[2])\n",
    "    fw.close()\n",
    "    \n",
    "    fw = open(\"SV.html\", \"w\")\n",
    "    fw.write(htmls[3])\n",
    "    fw.close()\n",
    "\n",
    "    fw = open(\"newwords.html\", \"w\")\n",
    "    fw.write(htmls[4])\n",
    "    fw.close()\n",
    "    \n",
    "    fw = open(\"qsvp.html\", \"w\")\n",
    "    fw.write(htmls[5])\n",
    "    fw.close()\n",
    "    \n",
    "    fw = open(\"qnew.html\", \"w\")\n",
    "    fw.write(htmls[6])\n",
    "    fw.close()    \n",
    "    fw = open(\"qadp.html\", \"w\")\n",
    "    fw.write(htmls[7])\n",
    "    fw.close()    \n",
    "    fw = open(\"qoth.html\", \"w\")\n",
    "    fw.write(htmls[8])\n",
    "    fw.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
